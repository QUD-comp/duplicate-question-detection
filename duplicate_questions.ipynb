{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 3)\n",
      "preprocessing complete\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# load the Quora questions dataset and preprocess to clean the data\n",
    "df = pd.read_csv(\"quora_duplicate_questions.tsv\",delimiter='\\t')\n",
    "df = df.drop(['id','qid1','qid2'],axis=1) # don't need the ID columns (ID also provided by DF object)\n",
    "\n",
    "to_drop = [] # list to drop any NaN or non-tokenizable rows found during scan of all tokens\n",
    "\n",
    "for i,row in df.iterrows():\n",
    "    try:\n",
    "        tok_text = word_tokenize(row['question1']) + word_tokenize(row['question2'])\n",
    "    except: # drop all rows which can't be tokenized by nltk, e.g. NaN\n",
    "        to_drop.append(i)\n",
    "        \n",
    "for i in to_drop:\n",
    "    df = df.drop([i])\n",
    "\n",
    "print(df.shape)\n",
    "print(\"preprocessing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "search for named entity overlap and add this ratio to df.  This can help identify content-similar questions, but \n",
    "will be tricked by non-duplicates with differing question words, e.g. \"how do I go from New York to Kentucky?\" vs.\n",
    "\"Why would I go from New York to Kentucky?\"\n",
    "'''\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "\n",
    "# add new column for entity overlap feature\n",
    "def add_entity_col(df):\n",
    "    df['entity overlap'] = np.zeros(len(df))\n",
    "    return df\n",
    "\n",
    "# takes the input texts and returns chunks\n",
    "def get_chunks(text):\n",
    "    return tree2conlltags(ne_chunk(pos_tag(word_tokenize(text))))\n",
    "\n",
    "# returns named entity chunks from all input chunks\n",
    "def get_named_entities(chunk):\n",
    "    entities = []\n",
    "    for token in chunk:\n",
    "        if token[2] != 'O':\n",
    "            entities.append(token[0])\n",
    "    return entities\n",
    "\n",
    "# takes list of entities and returns entity overlap / total number of entities\n",
    "def get_entity_overlap(list_one,list_two):\n",
    "    num_entities = len(list_one+list_two)\n",
    "    count = 0\n",
    "    for entity in list_one:\n",
    "        if entity in list_two: # same entity in both NE lists\n",
    "            count +=1\n",
    "            \n",
    "    try: \n",
    "        ratio = count/num_entities\n",
    "    except: # catch Divide by 0 error in case there were no entities\n",
    "        ratio = 0\n",
    "        \n",
    "    return ratio # ratio of same entities to total number of entities in both lists\n",
    "\n",
    "# add the named entity column to the df and fill in its value\n",
    "def fill_named_entity(df):\n",
    "    df = add_entity_col(df)\n",
    "    \n",
    "    for i,row in df.iterrows():\n",
    "        chunk_one = get_chunks(row['question1'])\n",
    "        list_one = get_named_entities(chunk_one)\n",
    "    \n",
    "        chunk_two = get_chunks(row['question2'])\n",
    "        list_two = get_named_entities(chunk_two)\n",
    "    \n",
    "        entity_overlap = get_entity_overlap(list_one,list_two)\n",
    "    \n",
    "        df.loc[i,'entity overlap'] = entity_overlap\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "search for question words overlap and add this radio to df.  This can help to combat the problem listed above, i.e.\n",
    "pairs which use the same question words are more likely to be going after the same answer.\n",
    "question words = what/which, when, where, who/whom, why, how\n",
    "'''\n",
    "# adds a new column for each question word in the df\n",
    "def add_question_cols(df):\n",
    "    question_words = ['what/which','when','where','who/whom','why','how']\n",
    "\n",
    "    for word in question_words:            \n",
    "        df[word] = np.zeros(len(df))\n",
    "    return df, question_words\n",
    "\n",
    "# fill in the question word columns for each row with occurrences of question word / total number of words\n",
    "def fill_question_words(df):\n",
    "    df, question_words = add_question_cols(df)\n",
    "    \n",
    "    for i,row in df.iterrows():\n",
    "        one_tok = word_tokenize(row['question1'])\n",
    "        one_tok = [word.lower() for word in one_tok]\n",
    "        two_tok = word_tokenize(row['question2'])\n",
    "        two_tok = [word.lower() for word in two_tok]\n",
    "        \n",
    "        num_words = len(one_tok+two_tok)\n",
    "        \n",
    "        for question_word in question_words:\n",
    "            occurrences = 0\n",
    "            if '/' in question_word: # check for multiple words under one category, e.g. who/whom, and search for both\n",
    "                word_list = question_word.split('/')\n",
    "                for word in word_list:\n",
    "                    occurrences += one_tok.count(word) + two_tok.count(word)\n",
    "            else:\n",
    "                occurrences = one_tok.count(question_word) + two_tok.count(question_word)\n",
    "            df.loc[i,question_word] = occurrences/num_words\n",
    "        \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "search for n-gram similarity features and add them to the df.  These features will also pick up on named entities\n",
    "and question words like above.  Jaccard distance and cosine similarity added to df.\n",
    "'''\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# adds 2 columns, Jaccard and cosine, for each n-gram value in range 1-6\n",
    "def add_ngram_cols(df):\n",
    "    n_gram = list(range(1,7))\n",
    "    \n",
    "    for gram in n_gram:\n",
    "        df[str(gram)+\"-gram cosine\"] = np.zeros(len(df))\n",
    "        df[str(gram)+\"-gram jaccard\"] = np.zeros(len(df))\n",
    "    return df, n_gram\n",
    "    \n",
    "def get_ngrams(text, gram):\n",
    "    grams = list(ngrams(text.split(), gram,pad_left=True,pad_right=True))\n",
    "    return grams\n",
    "\n",
    "def cosine_similarity_ngrams(a, b):\n",
    "    vec1 = Counter(a)\n",
    "    vec2 = Counter(b)\n",
    "    \n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    return float(numerator) / denominator\n",
    "\n",
    "def jaccard_distance_ngrams(a, b):\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "    return 1.0 * len(a&b)/len(a|b)\n",
    "\n",
    "# fill in the Jaccard distance and cosine similarity for each ngram 1-6\n",
    "def fill_ngrams(df):\n",
    "    df, n_gram = add_ngram_cols(df)\n",
    "    \n",
    "    for i,row in df.iterrows():\n",
    "        for gram in n_gram:\n",
    "            one_ngrams = get_ngrams(row['question1'],gram)\n",
    "            two_ngrams = get_ngrams(row['question2'],gram)\n",
    "            cosine_similarity = cosine_similarity_ngrams(one_ngrams,two_ngrams)\n",
    "            jaccard_distance = jaccard_distance_ngrams(one_ngrams,two_ngrams)\n",
    "        \n",
    "            df.loc[i,str(gram)+\"-gram cosine\"] = cosine_similarity\n",
    "            df.loc[i,str(gram)+\"-gram jaccard\"] = jaccard_distance\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "iterate through df and fill columns.\n",
    "NOTE: runtime of about 1 day to generate complete dataset with all features added\n",
    "'''\n",
    "import datetime as dt\n",
    "\n",
    "def fill_dataframe(df,filename):\n",
    "    print(\"starting \",filename)\n",
    "    \n",
    "    start = dt.datetime.now()\n",
    "    \n",
    "    df = fill_named_entity(df)\n",
    "    print(\"named entities complete for \",filename,\", took \",dt.datetime.now()-start, \" seconds\")\n",
    "    print(df.head())\n",
    "    \n",
    "    df = fill_question_words(df)\n",
    "    print(\"question words complete for \",filename,\", took \",dt.datetime.now()-start, \" seconds\")\n",
    "    print(df.head())\n",
    "    df = fill_ngrams(df)\n",
    "    print(\"n-grams complete for \",filename,\", took \",dt.datetime.now()-start, \" seconds\")\n",
    "    print(df.head())\n",
    "    end = dt.datetime.now()\n",
    "    print(filename,\" complete, it took \",end-start,\" seconds\")\n",
    "    print(df.head())\n",
    "    df.to_csv(filename,sep=\"|\",index=False)\n",
    "    print(\"file saved\")\n",
    "    \n",
    "fill_dataframe(df,'duplicate_questions.csv') # the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  duplicate_questions_spellcheck.csv\n",
      "named entities complete for  duplicate_questions_spellcheck.csv , took  1:48:07.698616  seconds\n",
      "                                           question1  \\\n",
      "0  What is the step by step guide to invest in sh...   \n",
      "1  What is the story of Kohinoor e Oh-i-Noor e Di...   \n",
      "2  How can I increase the speed of my INTERNET co...   \n",
      "3  Why am I mentally very lonely? How can I solve...   \n",
      "4  Which one dissolve in water quickly sugar e sa...   \n",
      "\n",
      "                                           question2  is_duplicate  \\\n",
      "0  What is the step by step guide to invest in sh...             0   \n",
      "1  What would happen if the Indian government sto...             0   \n",
      "2  How can Internet speed be increased by hacking...             0   \n",
      "3  Find the remainder when e math e 23^ e 24 e e ...             0   \n",
      "4            Which fish would survive in salt water?             0   \n",
      "\n",
      "   entity overlap  \n",
      "0            0.00  \n",
      "1            0.25  \n",
      "2            0.00  \n",
      "3            0.00  \n",
      "4            0.00  \n",
      "question words complete for  duplicate_questions_spellcheck.csv , took  3:25:34.109219  seconds\n",
      "                                           question1  \\\n",
      "0  What is the step by step guide to invest in sh...   \n",
      "1  What is the story of Kohinoor e Oh-i-Noor e Di...   \n",
      "2  How can I increase the speed of my INTERNET co...   \n",
      "3  Why am I mentally very lonely? How can I solve...   \n",
      "4  Which one dissolve in water quickly sugar e sa...   \n",
      "\n",
      "                                           question2  is_duplicate  \\\n",
      "0  What is the step by step guide to invest in sh...             0   \n",
      "1  What would happen if the Indian government sto...             0   \n",
      "2  How can Internet speed be increased by hacking...             0   \n",
      "3  Find the remainder when e math e 23^ e 24 e e ...             0   \n",
      "4            Which fish would survive in salt water?             0   \n",
      "\n",
      "   entity overlap  what/which     when  where  who/whom      why       how  \n",
      "0            0.00    0.071429  0.00000    0.0       0.0  0.00000  0.000000  \n",
      "1            0.25    0.074074  0.00000    0.0       0.0  0.00000  0.000000  \n",
      "2            0.00    0.000000  0.00000    0.0       0.0  0.00000  0.076923  \n",
      "3            0.00    0.000000  0.03125    0.0       0.0  0.03125  0.031250  \n",
      "4            0.00    0.083333  0.00000    0.0       0.0  0.00000  0.000000  \n",
      "ngrams complete for  duplicate_questions_spellcheck.csv , took  7:35:25.896311  seconds\n",
      "                                           question1  \\\n",
      "0  What is the step by step guide to invest in sh...   \n",
      "1  What is the story of Kohinoor e Oh-i-Noor e Di...   \n",
      "2  How can I increase the speed of my INTERNET co...   \n",
      "3  Why am I mentally very lonely? How can I solve...   \n",
      "4  Which one dissolve in water quickly sugar e sa...   \n",
      "\n",
      "                                           question2  is_duplicate  \\\n",
      "0  What is the step by step guide to invest in sh...             0   \n",
      "1  What would happen if the Indian government sto...             0   \n",
      "2  How can Internet speed be increased by hacking...             0   \n",
      "3  Find the remainder when e math e 23^ e 24 e e ...             0   \n",
      "4            Which fish would survive in salt water?             0   \n",
      "\n",
      "   entity overlap  what/which     when  where  who/whom      why       how  \\\n",
      "0            0.00    0.071429  0.00000    0.0       0.0  0.00000  0.000000   \n",
      "1            0.25    0.074074  0.00000    0.0       0.0  0.00000  0.000000   \n",
      "2            0.00    0.000000  0.00000    0.0       0.0  0.00000  0.076923   \n",
      "3            0.00    0.000000  0.03125    0.0       0.0  0.03125  0.031250   \n",
      "4            0.00    0.083333  0.00000    0.0       0.0  0.00000  0.000000   \n",
      "\n",
      "        ...        2-gram cosine  2-gram jaccard  3-gram cosine  \\\n",
      "0       ...             0.787726        0.647059       0.734968   \n",
      "1       ...             0.301511        0.173913       0.210042   \n",
      "2       ...             0.155700        0.083333       0.144338   \n",
      "3       ...             0.000000        0.000000       0.000000   \n",
      "4       ...             0.088388        0.043478       0.080845   \n",
      "\n",
      "   3-gram jaccard  4-gram cosine  4-gram jaccard  5-gram cosine  \\\n",
      "0        0.578947       0.688847        0.523810       0.648181   \n",
      "1        0.115385       0.130744        0.068966       0.061314   \n",
      "2        0.076923       0.134535        0.071429       0.125988   \n",
      "3        0.000000       0.000000        0.000000       0.000000   \n",
      "4        0.040000       0.074536        0.037037       0.069171   \n",
      "\n",
      "   5-gram jaccard  6-gram cosine  6-gram jaccard  \n",
      "0        0.478261       0.612056        0.440000  \n",
      "1        0.031250       0.057735        0.029412  \n",
      "2        0.066667       0.118470        0.062500  \n",
      "3        0.000000       0.000000        0.000000  \n",
      "4        0.034483       0.064550        0.032258  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "duplicate_questions_spellcheck.csv  complete, it took  7:35:25.961281  seconds\n",
      "                                           question1  \\\n",
      "0  What is the step by step guide to invest in sh...   \n",
      "1  What is the story of Kohinoor e Oh-i-Noor e Di...   \n",
      "2  How can I increase the speed of my INTERNET co...   \n",
      "3  Why am I mentally very lonely? How can I solve...   \n",
      "4  Which one dissolve in water quickly sugar e sa...   \n",
      "\n",
      "                                           question2  is_duplicate  \\\n",
      "0  What is the step by step guide to invest in sh...             0   \n",
      "1  What would happen if the Indian government sto...             0   \n",
      "2  How can Internet speed be increased by hacking...             0   \n",
      "3  Find the remainder when e math e 23^ e 24 e e ...             0   \n",
      "4            Which fish would survive in salt water?             0   \n",
      "\n",
      "   entity overlap  what/which     when  where  who/whom      why       how  \\\n",
      "0            0.00    0.071429  0.00000    0.0       0.0  0.00000  0.000000   \n",
      "1            0.25    0.074074  0.00000    0.0       0.0  0.00000  0.000000   \n",
      "2            0.00    0.000000  0.00000    0.0       0.0  0.00000  0.076923   \n",
      "3            0.00    0.000000  0.03125    0.0       0.0  0.03125  0.031250   \n",
      "4            0.00    0.083333  0.00000    0.0       0.0  0.00000  0.000000   \n",
      "\n",
      "        ...        2-gram cosine  2-gram jaccard  3-gram cosine  \\\n",
      "0       ...             0.787726        0.647059       0.734968   \n",
      "1       ...             0.301511        0.173913       0.210042   \n",
      "2       ...             0.155700        0.083333       0.144338   \n",
      "3       ...             0.000000        0.000000       0.000000   \n",
      "4       ...             0.088388        0.043478       0.080845   \n",
      "\n",
      "   3-gram jaccard  4-gram cosine  4-gram jaccard  5-gram cosine  \\\n",
      "0        0.578947       0.688847        0.523810       0.648181   \n",
      "1        0.115385       0.130744        0.068966       0.061314   \n",
      "2        0.076923       0.134535        0.071429       0.125988   \n",
      "3        0.000000       0.000000        0.000000       0.000000   \n",
      "4        0.040000       0.074536        0.037037       0.069171   \n",
      "\n",
      "   5-gram jaccard  6-gram cosine  6-gram jaccard  \n",
      "0        0.478261       0.612056        0.440000  \n",
      "1        0.031250       0.057735        0.029412  \n",
      "2        0.066667       0.118470        0.062500  \n",
      "3        0.000000       0.000000        0.000000  \n",
      "4        0.034483       0.064550        0.032258  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "file saved\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "create a spell-checked version of the entire dataframe\n",
    "'''\n",
    "import hunspell\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "\n",
    "hobj = hunspell.HunSpell('en_US/en_US.dic', 'en_US/en_US.aff') # load the necessary dictionaries\n",
    "\n",
    "df_spellcheck = df.copy() # new df for spellchecked version of the entire dataframe\n",
    "\n",
    "# uses Hunspell to check each token against dictionary, replaces with most likely suggestion if token not found\n",
    "def get_spellchecked(text):\n",
    "    for index,token in enumerate(text):\n",
    "        try:\n",
    "            if token ==\"?\":\n",
    "                continue\n",
    "            is_correct = hobj.spell(token)\n",
    "            if not is_correct:\n",
    "                correction = hobj.suggest(token)[0]\n",
    "                text[index] = correction\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    detokenizer = MosesDetokenizer()\n",
    "    text = detokenizer.detokenize(text,return_str=True) # use detokenizer to get str version of text back\n",
    "    \n",
    "    return text\n",
    "\n",
    "# fill the new df with spellchecked text\n",
    "for i,row in df_spellcheck.iterrows():\n",
    "    one_corrected = get_spellchecked(word_tokenize(row['question1']))\n",
    "    df_spellcheck.loc[i,'question1'] = one_corrected\n",
    "    \n",
    "    two_corrected = get_spellchecked(word_tokenize(row['question2']))\n",
    "    df_spellcheck.loc[i,'question2'] = two_corrected\n",
    "    \n",
    "fill_dataframe(df_spellcheck,'duplicate_questions_spellcheck.csv') #the spellchecked dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINTING ACCURACY FOR  None\n",
      "69.245 %\n",
      "[('2-gram cosine', 2.8835302409187547), ('1-gram jaccard', 1.2473903254071896), ('6-gram jaccard', 0.4549311888569346)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['entity overlap']\n",
      "63.108 %\n",
      "[('entity overlap', 0.2144051319281528)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['what/which']\n",
      "63.130 %\n",
      "[('what/which', 0.07806939000196023)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['when']\n",
      "63.130 %\n",
      "[('when', -0.05607662217233648)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['where']\n",
      "63.130 %\n",
      "[('where', -0.02010545912858436)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['who/whom']\n",
      "63.130 %\n",
      "[('who/whom', 0.003651326480146271)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['why']\n",
      "63.135 %\n",
      "[('why', 0.07892364303259582)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['how']\n",
      "63.577 %\n",
      "[('how', 0.20180725517357606)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['what/which', 'when', 'where', 'who/whom', 'why', 'how']\n",
      "63.509 %\n",
      "[('how', 0.35629421406900247), ('what/which', 0.27796667705780065), ('why', 0.20166324692350385)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['1-gram cosine', '1-gram jaccard']\n",
      "64.770 %\n",
      "[('1-gram cosine', 2.1016236325910778), ('1-gram jaccard', -1.2658652680952776)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['1-gram cosine']\n",
      "64.773 %\n",
      "[('1-gram cosine', 0.8100392575175709)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['1-gram jaccard']\n",
      "63.916 %\n",
      "[('1-gram jaccard', 0.6918069687329219)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['2-gram cosine', '2-gram jaccard']\n",
      "67.116 %\n",
      "[('2-gram cosine', 4.024204890498794), ('2-gram jaccard', -3.397498275704922)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['2-gram cosine']\n",
      "63.464 %\n",
      "[('2-gram cosine', 0.6156715890516836)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['2-gram jaccard']\n",
      "62.702 %\n",
      "[('2-gram jaccard', 0.4995971396911132)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['3-gram cosine', '3-gram jaccard']\n",
      "66.407 %\n",
      "[('3-gram cosine', 3.895224127139987), ('3-gram jaccard', -3.401806686308141)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['3-gram cosine']\n",
      "62.935 %\n",
      "[('3-gram cosine', 0.5083078790901315)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['3-gram jaccard']\n",
      "62.326 %\n",
      "[('3-gram jaccard', 0.4143216242505568)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['4-gram cosine', '4-gram jaccard']\n",
      "66.040 %\n",
      "[('4-gram cosine', 3.831396951119188), ('4-gram jaccard', -3.394510699973291)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['4-gram cosine']\n",
      "62.751 %\n",
      "[('4-gram cosine', 0.4587717325014923)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['4-gram jaccard']\n",
      "62.242 %\n",
      "[('4-gram jaccard', 0.37701636392883625)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['5-gram cosine', '5-gram jaccard']\n",
      "66.012 %\n",
      "[('5-gram cosine', 3.844416064883646), ('5-gram jaccard', -3.4336763389847493)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['5-gram cosine']\n",
      "62.590 %\n",
      "[('5-gram cosine', 0.4354065949469396)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['5-gram jaccard']\n",
      "62.132 %\n",
      "[('5-gram jaccard', 0.360532669789091)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['6-gram cosine', '6-gram jaccard']\n",
      "65.853 %\n",
      "[('6-gram cosine', 3.896545499196161), ('6-gram jaccard', -3.4992655323228954)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['6-gram cosine']\n",
      "62.595 %\n",
      "[('6-gram cosine', 0.42349435182340534)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['6-gram jaccard']\n",
      "62.068 %\n",
      "[('6-gram jaccard', 0.35300523055923183)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['1-gram cosine', '2-gram cosine', '3-gram cosine', '4-gram cosine', '5-gram cosine', '6-gram cosine']\n",
      "65.651 %\n",
      "[('1-gram cosine', 1.2405866019669918), ('2-gram cosine', 0.6332310757787807), ('5-gram cosine', 0.16776036840724792)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['1-gram jaccard', '2-gram jaccard', '3-gram jaccard', '4-gram jaccard', '5-gram jaccard', '6-gram jaccard']\n",
      "66.696 %\n",
      "[('1-gram jaccard', 1.8370421918136954), ('6-gram jaccard', 0.3561560045955017), ('2-gram jaccard', -0.07222636258877253)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['1-gram cosine', '2-gram cosine', '3-gram cosine', '4-gram cosine', '5-gram cosine', '6-gram cosine', '1-gram jaccard', '2-gram jaccard', '3-gram jaccard', '4-gram jaccard', '5-gram jaccard', '6-gram jaccard']\n",
      "68.689 %\n",
      "[('2-gram cosine', 2.9935143007790694), ('1-gram jaccard', 1.2998919937227522), ('6-gram jaccard', 0.42740905990653705)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  None\n",
      "69.307 %\n",
      "[('2-gram cosine', 2.6890770694622805), ('1-gram jaccard', 1.4325532500113192), ('6-gram jaccard', 1.1843266409918642)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['entity overlap']\n",
      "63.094 %\n",
      "[('entity overlap', 0.23267920180700477)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['what/which']\n",
      "63.130 %\n",
      "[('what/which', 0.07903166506413549)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['when']\n",
      "63.130 %\n",
      "[('when', -0.05563501514245879)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['where']\n",
      "63.130 %\n",
      "[('where', -0.01987165931709591)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['who/whom']\n",
      "63.130 %\n",
      "[('who/whom', 0.004143984452465069)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['why']\n",
      "63.144 %\n",
      "[('why', 0.0797268371569746)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['how']\n",
      "63.555 %\n",
      "[('how', 0.20291731778571315)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['what/which', 'when', 'where', 'who/whom', 'why', 'how']\n",
      "63.487 %\n",
      "[('how', 0.35823184501740657), ('what/which', 0.27968926782925785), ('why', 0.20287081198028864)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['1-gram cosine', '1-gram jaccard']\n",
      "64.773 %\n",
      "[('1-gram cosine', 1.712576949474584), ('1-gram jaccard', -0.8882338783196991)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['1-gram cosine']\n",
      "64.964 %\n",
      "[('1-gram cosine', 0.8120508337421556)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['1-gram jaccard']\n",
      "64.237 %\n",
      "[('1-gram jaccard', 0.7049940483840499)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['2-gram cosine', '2-gram jaccard']\n",
      "67.009 %\n",
      "[('2-gram cosine', 3.8021452743045647), ('2-gram jaccard', -3.160172165776189)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['2-gram cosine']\n",
      "63.751 %\n",
      "[('2-gram cosine', 0.6313659947093342)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['2-gram jaccard']\n",
      "62.920 %\n",
      "[('2-gram jaccard', 0.5163776678942226)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['3-gram cosine', '3-gram jaccard']\n",
      "66.562 %\n",
      "[('3-gram cosine', 3.705159224404302), ('3-gram jaccard', -3.1964377597773526)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['3-gram cosine']\n",
      "63.160 %\n",
      "[('3-gram cosine', 0.526109172389116)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['3-gram jaccard']\n",
      "62.495 %\n",
      "[('3-gram jaccard', 0.4315446292174065)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['4-gram cosine', '4-gram jaccard']\n",
      "66.206 %\n",
      "[('4-gram cosine', 3.6125712761987505), ('4-gram jaccard', -3.164722720003022)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['4-gram cosine']\n",
      "62.937 %\n",
      "[('4-gram cosine', 0.4759013708752557)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['4-gram jaccard']\n",
      "62.407 %\n",
      "[('4-gram jaccard', 0.39322735640290873)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['5-gram cosine', '5-gram jaccard']\n",
      "66.142 %\n",
      "[('5-gram cosine', 3.5505016712375346), ('5-gram jaccard', -3.134366538988442)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['5-gram cosine']\n",
      "62.787 %\n",
      "[('5-gram cosine', 0.4508469312949472)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['5-gram jaccard']\n",
      "62.322 %\n",
      "[('5-gram jaccard', 0.37533637391836067)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['6-gram cosine', '6-gram jaccard']\n",
      "66.004 %\n",
      "[('6-gram cosine', 3.521943513352268), ('6-gram jaccard', -3.124800293304691)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['6-gram cosine']\n",
      "62.803 %\n",
      "[('6-gram cosine', 0.43792683516339975)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['6-gram jaccard']\n",
      "62.214 %\n",
      "[('6-gram jaccard', 0.3670913921544154)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['1-gram cosine', '2-gram cosine', '3-gram cosine', '4-gram cosine', '5-gram cosine', '6-gram cosine']\n",
      "65.612 %\n",
      "[('1-gram cosine', 1.1187950291018471), ('2-gram cosine', 0.7211789926869973), ('4-gram cosine', 0.12896552959743973)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['1-gram jaccard', '2-gram jaccard', '3-gram jaccard', '4-gram jaccard', '5-gram jaccard', '6-gram jaccard']\n",
      "66.780 %\n",
      "[('1-gram jaccard', 1.7767586084842408), ('6-gram jaccard', 0.4863944384762981), ('2-gram jaccard', -0.051781226511683716)]\n",
      "\n",
      "\n",
      "PRINTING ACCURACY FOR  ['1-gram cosine', '2-gram cosine', '3-gram cosine', '4-gram cosine', '5-gram cosine', '6-gram cosine', '1-gram jaccard', '2-gram jaccard', '3-gram jaccard', '4-gram jaccard', '5-gram jaccard', '6-gram jaccard']\n",
      "68.532 %\n",
      "[('2-gram cosine', 2.8133455065164017), ('1-gram jaccard', 1.4819854631210758), ('6-gram jaccard', 1.191511035930424)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import operator\n",
    "\n",
    "# gets 20 misclassified instances and prints them to file, including model's prediction\n",
    "def get_error_analysis(X,y):\n",
    "    file = open('error_analysis.txt','w')\n",
    "    df = pd.read_csv('duplicate_questions_spellcheck.csv',delimiter='|')\n",
    "    compare = list(zip(X,y))\n",
    "    misclassifications = []\n",
    "    for i,(X,y) in enumerate(compare):\n",
    "        if X != y:\n",
    "            misclassifications.append(i)\n",
    "    \n",
    "    indices = np.random.randint(0,high=len(misclassifications),size=20)\n",
    "    \n",
    "    for index in indices:\n",
    "        text_one = df.loc[index,'question1']\n",
    "        text_two = df.loc[index,'question2']\n",
    "        file.write(text_one+\"\\n\"+text_two+\"\\n\")\n",
    "        file.write(\"(model predicted \"+str(compare[index][0])+\")\\n\\n\")\n",
    "    file.close()\n",
    "\n",
    "def regression_results(df,features):\n",
    "    print(\"PRINTING ACCURACY FOR \",features,)\n",
    "    filters = ['question1','question2']\n",
    "    y = df['is_duplicate']\n",
    "    df = df.drop(['is_duplicate'],axis=1)\n",
    "    \n",
    "    cols = []\n",
    "    \n",
    "    if features is not None:\n",
    "        cols = list(df.columns)\n",
    "        for feature in features:\n",
    "            cols.remove(feature)\n",
    "    \n",
    "    filters += cols\n",
    "    \n",
    "    X = df[df.columns.difference(filters)] # filter out the text of the questions for analysis\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    logisticRegr = LogisticRegression()\n",
    "    logisticRegr.fit(X_train, y_train)\n",
    "    predictions = logisticRegr.predict(X_test)\n",
    "    \n",
    "    if features == None:\n",
    "        get_error_analysis(predictions,y_test)\n",
    "    \n",
    "    score = logisticRegr.score(X_test, y_test)\n",
    "    print(\"%.3f\" % (score*100),\"%\")\n",
    "\n",
    "    cols = X.columns\n",
    "    stdev = np.std(X_train, 0)\n",
    "    stdev_vals = [stdev[i] for i,val in enumerate(stdev)]\n",
    "    stdev_vals = stdev_vals*logisticRegr.coef_\n",
    "    for i,val in enumerate(stdev_vals[0]):\n",
    "        stdev[i] = val\n",
    "\n",
    "    col_stdev_dict = dict([k,v] for k,v in list(zip(cols,stdev)))\n",
    "    print(sorted(col_stdev_dict.items(), key=operator.itemgetter(1),reverse=True)[:3])\n",
    "    print(\"\\n\")\n",
    "\n",
    "files = ['duplicate_questions.csv','duplicate_questions_spellcheck.csv']\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(file,delimiter='|')\n",
    "    \n",
    "    #overall accuracy results\n",
    "    regression_results(df,None)\n",
    "\n",
    "    #entity overlap accuracy\n",
    "    regression_results(df,['entity overlap'])\n",
    "\n",
    "    #question word accuracy\n",
    "    question_words = ['what/which','when','where','who/whom','why','how']\n",
    "    # each word separately\n",
    "    for word in question_words:\n",
    "        regression_results(df,[word])\n",
    "    \n",
    "    #all question words together\n",
    "    regression_results(df,question_words)\n",
    "\n",
    "    #ngram accuracy by gram (cosine and jaccardian), and each measure separately\n",
    "    ngrams = ['1-gram','2-gram','3-gram','4-gram','5-gram','6-gram']\n",
    "    for gram in ngrams:\n",
    "        regression_results(df,[gram+' cosine',gram+' jaccard'])\n",
    "        regression_results(df,[gram+' cosine'])\n",
    "        regression_results(df,[gram+' jaccard'])\n",
    "\n",
    "    # all ngrams together\n",
    "    jaccard_list = []\n",
    "    cosine_list = []\n",
    "    for gram in ngrams:\n",
    "        jaccard_list.append(gram+' jaccard')\n",
    "        cosine_list.append(gram+' cosine')\n",
    "    regression_results(df,cosine_list)\n",
    "    regression_results(df,jaccard_list)\n",
    "    regression_results(df,cosine_list+jaccard_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewbeaton/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-655c71ffc73f>:157: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Initialized\n",
      "Average loss at step  0 :  9.345418930053711\n",
      "Nearest to UNK: reputation, commit, east, banned, Dual, object-oriented, remotely, flagged,\n",
      "Nearest to if: pleasure, weights, induction, prevalent, logic, Power, matches, calcium,\n",
      "Nearest to Who: Seals, regional, innovative, engaged, Course, Comics, Ellen, prescribe,\n",
      "Nearest to think: KP, nude, progress, conclusion, kit, justice, registration, bark,\n",
      "Nearest to good: yogurt, publishing, mind-blowing, circles, baseball, officially, insane, depression,\n",
      "Nearest to all: voted, classified, 64, AIMS, Metal, laptops, advertise, unexpected,\n",
      "Nearest to I: market, participate, everyone, doctor, Bier, holidays, expenditure, tower,\n",
      "Nearest to money: Energy, dreaming, Chechen, minute, Somme, provides, Don, TATS,\n",
      "Nearest to by: legacy, Brazil, passports, Archaic, tears, Cash, lists, alternate,\n",
      "Nearest to your: consulting, lions, metaphor, libertarian, Hindus, artwork, 2011, fractions,\n",
      "Nearest to time: question, Muslims, blow, aluminum, Riggs, meters, normal, Mississippi,\n",
      "Nearest to it: 1990, alliance, 13, incidents, Churchill, tomato, surgery, Spirit,\n",
      "Nearest to as: wolves, 29, strip, handling, HTTP, bonding, November, Grey,\n",
      "Nearest to they: BS, feature, promised, update, Dravidian, Person, responsibility, behind,\n",
      "Nearest to my: elementary, boo, Bowl, Armstrong, kit, brings, She, funny,\n",
      "Nearest to use: repeat, performing, D, offensive, stuffed, Raven, profitable, Turbo,\n",
      "Average loss at step  2000 :  9.371399909496308\n",
      "Average loss at step  4000 :  9.370682838916778\n",
      "Average loss at step  6000 :  9.366900543689727\n",
      "Average loss at step  8000 :  9.368297228336335\n",
      "Average loss at step  10000 :  9.365897017002105\n",
      "Nearest to UNK: e, sqrt, diploma, Internet, history, world, deserts, slot,\n",
      "Nearest to if: things, destroy, when, weights, strengths, still, f, article,\n",
      "Nearest to Who: What, How, Why, Seals, International, exist, wisdom, care,\n",
      "Nearest to think: nude, justice, bark, progress, PPM, Odis, wonderful, enemies,\n",
      "Nearest to good: mind-blowing, publishing, baseball, yogurt, making, insane, depression, easiest,\n",
      "Nearest to all: voted, 64, laptops, unexpected, classified, advertise, fare, AIMS,\n",
      "Nearest to I: you, we, to, watches, everyone, Where, God, research,\n",
      "Nearest to money: Chechen, Energy, girls, dreaming, Where, Ms, PET, Access,\n",
      "Nearest to by: Brazil, passports, legacy, enemies, in, master, alternate, GPA,\n",
      "Nearest to your: my, the, artwork, Rumba, translate, social, lions, awesome,\n",
      "Nearest to time: Muslims, demonetization, aluminum, normal, tie, uniforms, blow, Iran,\n",
      "Nearest to it: this, 1990, tomato, President, firewall, slot, liquid, 13,\n",
      "Nearest to as: handling, 29, bonding, months, rarely, message, answer, purchasing,\n",
      "Nearest to they: you, reply, we, Dravidian, always, I, BS, feature,\n",
      "Nearest to my: your, a, the, deaf, Bowl, ceremony, implementation, Jaipur,\n",
      "Nearest to use: D, meaning, repeat, get, performing, Raven, optical, quit,\n",
      "Average loss at step  12000 :  9.36600899887085\n",
      "Average loss at step  14000 :  9.366144648075103\n",
      "Average loss at step  16000 :  9.363889081954955\n",
      "Average loss at step  18000 :  9.368201012134552\n",
      "Average loss at step  20000 :  9.366247178554534\n",
      "Nearest to UNK: e, ``, deserts, Huh, diploma, sqrt, Internet, slot,\n",
      "Nearest to if: when, destroy, weights, still, to, will, things, Ba,\n",
      "Nearest to Who: What, How, Why, Which, Seals, wisdom, accessories, International,\n",
      "Nearest to think: nude, justice, Odis, prefer, PPM, progress, bark, demonetization,\n",
      "Nearest to good: mind-blowing, baseball, making, insane, publishing, popular, Personal, cigarette,\n",
      "Nearest to all: voted, laptops, 64, unexpected, advertise, classified, Metal, sex,\n",
      "Nearest to I: you, we, to, they, can, watches, Where, everyone,\n",
      "Nearest to money: Chechen, girls, dreaming, Energy, Where, acrylic, Ms, Access,\n",
      "Nearest to by: Brazil, enemies, passports, legacy, Cash, alternate, master, from,\n",
      "Nearest to your: my, the, deaf, Rumba, translate, entire, artwork, a,\n",
      "Nearest to time: demonetization, Muslims, aluminum, uniforms, normal, volcanoes, addict, question,\n",
      "Nearest to it: this, 1990, Donald, President, UNK, tomato, firewall, 30s,\n",
      "Nearest to as: 29, freshmen, handling, priced, contributed, included, message, answer,\n",
      "Nearest to they: you, we, I, always, reply, Dravidian, reserve, schedule,\n",
      "Nearest to my: your, a, deaf, the, fleas, their, Bowl, ceremony,\n",
      "Nearest to use: meaning, D, get, repeat, Raven, optical, performing, quit,\n",
      "Average loss at step  22000 :  9.365075578689575\n",
      "Average loss at step  24000 :  9.36449844121933\n",
      "Average loss at step  26000 :  9.36351854467392\n",
      "Average loss at step  28000 :  9.364303676605225\n",
      "Average loss at step  30000 :  9.364061985969544\n",
      "Nearest to UNK: Huh, ``, e, deserts, diploma, cupcake, slot, Louisiana,\n",
      "Nearest to if: when, will, destroy, can, is, to, solver, gotten,\n",
      "Nearest to Who: What, How, Why, Which, Where, accessories, wisdom, TX,\n",
      "Nearest to think: nude, justice, prefer, Odis, personally, demonetization, progress, PPM,\n",
      "Nearest to good: making, mind-blowing, baseball, insane, cigarette, publishing, Personal, popular,\n",
      "Nearest to all: voted, laptops, unexpected, advertise, 64, recruiters, Metal, classified,\n",
      "Nearest to I: you, we, to, can, they, watches, everyone, bombing,\n",
      "Nearest to money: girls, Chechen, dreaming, Where, Energy, lease, acrylic, provides,\n",
      "Nearest to by: Brazil, enemies, is, legacy, Cash, when, master, in,\n",
      "Nearest to your: my, the, their, deaf, Rumba, translate, entire, a,\n",
      "Nearest to time: demonetization, Muslims, uniforms, aluminum, volcanoes, normal, addict, kings,\n",
      "Nearest to it: this, 1990, Donald, he, UNK, President, liquid, 30s,\n",
      "Nearest to as: description, really, contributed, answer, priced, be, Isaac, freshmen,\n",
      "Nearest to they: you, we, I, always, reply, Dravidian, schedule, Ta,\n",
      "Nearest to my: your, their, a, our, his, the, deaf, fleas,\n",
      "Nearest to use: meaning, D, get, repeat, quit, Raven, optical, performing,\n",
      "Average loss at step  32000 :  9.363403827667236\n",
      "Average loss at step  34000 :  9.361834465026856\n",
      "Average loss at step  36000 :  9.359650051116944\n",
      "Average loss at step  38000 :  9.361591017723084\n",
      "Average loss at step  40000 :  9.361298532962799\n",
      "Nearest to UNK: diploma, e, ``, Huh, Internet, sqrt, Louisiana, deserts,\n",
      "Nearest to if: when, will, destroy, can, whenever, circuitry, gotten, though,\n",
      "Nearest to Who: What, How, Why, Which, Where, accessories, TX, innovative,\n",
      "Nearest to think: nude, prefer, justice, Odis, miss, personally, type, conclusion,\n",
      "Nearest to good: making, mind-blowing, best, baseball, insane, weapon, cigarette, publishing,\n",
      "Nearest to all: voted, unexpected, laptops, advertise, recruiters, poker, Metal, sex,\n",
      "Nearest to I: you, we, to, can, they, everyone, watches, deserts,\n",
      "Nearest to money: girls, dreaming, Chechen, Where, lease, Don, acrylic, Energy,\n",
      "Nearest to by: Brazil, enemies, Cash, when, legacy, Archaic, master, be,\n",
      "Nearest to your: my, the, their, deaf, his, our, social, Rumba,\n",
      "Nearest to time: demonetization, uniforms, Muslims, aluminum, normal, addict, volcanoes, kings,\n",
      "Nearest to it: 1990, this, he, Donald, President, she, liquid, 30s,\n",
      "Nearest to as: really, contributed, ancient, description, be, freshmen, answer, included,\n",
      "Nearest to they: you, we, I, always, reply, he, Donald, schedule,\n",
      "Nearest to my: your, his, their, our, a, the, deaf, fleas,\n",
      "Nearest to use: meaning, D, get, repeat, quit, Raven, prove, questionnaire,\n",
      "Average loss at step  42000 :  9.35676343011856\n",
      "Average loss at step  44000 :  9.358962206363678\n",
      "Average loss at step  46000 :  9.358345654010773\n",
      "Average loss at step  48000 :  9.358793551921844\n",
      "NCE method took 6166.457863 seconds to run 50000 iterations\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tensorflow implementation\n",
    "Tutorial provided by:\n",
    "http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from collections import Counter, deque\n",
    "import random\n",
    "\n",
    "# get a list of all words in the dataframe for word2vec processing, and call build_dataset to extract n most \n",
    "# common words and assign unique integer to each word.  Returns dicts which can look up integer-word and \n",
    "# word-integer pairs.\n",
    "def get_data(df,vocabulary_size):\n",
    "    vocabulary = []\n",
    "    \n",
    "    for i,row in df.iterrows():\n",
    "        tok_text = word_tokenize(row['question1']) + word_tokenize(row['question2'])\n",
    "        vocabulary += tok_text\n",
    "                                    \n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                                vocabulary_size)\n",
    "    del vocabulary  # Hint to reduce memory.\n",
    "\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "# Extract the n most common words from each level's texts.  \n",
    "# Credit to http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/\n",
    "def build_dataset(words, n_words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "data_index = 0\n",
    "# generate batch data\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    buffer = deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, context\n",
    "\n",
    "vocabulary_size = 10000\n",
    "data, count, dictionary, reverse_dictionary = get_data(df, vocabulary_size=vocabulary_size)\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a context\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "def run(graph, num_steps):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # We must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        print('Initialized')\n",
    "\n",
    "        average_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batch_inputs, batch_context = generate_batch(data,\n",
    "                batch_size, num_skips, skip_window)\n",
    "            feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
    "\n",
    "            # We perform one update step by evaluating the optimizer op (including it\n",
    "            # in the list of returned values for session.run()\n",
    "            _, loss_val = session.run([optimizer, cross_entropy], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "    \n",
    "                # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            if step % 10000 == 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                    top_k = 8  # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                    log_str = 'Nearest to %s:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = reverse_dictionary[nearest[k]]\n",
    "                        log_str = '%s %s,' % (log_str, close_word)\n",
    "                    print(log_str)\n",
    "        \n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "            \n",
    "with graph.as_default():\n",
    "\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    \n",
    "    # Construct the variables for the softmax\n",
    "    weights = tf.Variable(\n",
    "    tf.truncated_normal([embedding_size, vocabulary_size],\n",
    "                          stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    hidden_out = tf.transpose(tf.matmul(tf.transpose(weights), tf.transpose(embed))) + biases\n",
    "\n",
    "    # convert train_context to a one-hot format\n",
    "    train_one_hot = tf.one_hot(train_context, vocabulary_size)\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, labels=train_one_hot))\n",
    "    \n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    nce_loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_context,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "num_steps = 50000\n",
    "nce_start_time = dt.datetime.now()\n",
    "run(graph,num_steps)\n",
    "nce_end_time = dt.datetime.now()\n",
    "print(\"NCE method took {} seconds to run 50000 iterations\".format((nce_end_time-nce_start_time).total_seconds()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
